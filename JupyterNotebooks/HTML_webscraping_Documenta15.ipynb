{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for scraping **news items** from the **DOCUMENTA FIFTEEN WEBSITE**: https://documenta-fifteen.de/en/\n",
        "\n",
        "First of all, you need to connect this Colab notebook with your Google Drive and define the directory for input and output data."
      ],
      "metadata": {
        "id": "VMM3wdd5TbHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_NLP_UM/\""
      ],
      "metadata": {
        "id": "v6wx6ArFTmAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to install additional packages."
      ],
      "metadata": {
        "id": "dMdZGYkiUEBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install BeautifulSoup\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "E4QxcXlaTmLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can import the packages and run the actual code. In the **first section**, the script will navigate to the DOCUMENTA15 news site and identity the links to individual news items.\n",
        "\n",
        "It will also download the individual news pages as HTML files for offline use and store them in a Google Drive folder.\n",
        "\n",
        "The page where all news items are linked is: https://documenta-fifteen.de/en/news/\n",
        "\n",
        "The links to each news item are hidden in an \"unordered list\" within the HTML source code:\n",
        "\n",
        "```\n",
        "<ul class=\"filter-list\">\n",
        "<li class=\"filter-list__item\" x-show=\"isFiltered('general ')\">\n",
        "<a  @mouseenter=\"{\n",
        "                        $refs.previewImage.src = 'https://documenta-fifteen.de/wp-content/uploads/2022/09/d15_Fridericianum_2_2022_09_24_Â©_Nicolas_Wefers_001.jpg'; \n",
        "                        $refs.previewImageWrap.classList.add('active');\n",
        "                    }\" \n",
        "    @mouseleave=\"$refs.previewImageWrap.classList.remove('active');\"  href=\"https://documenta-fifteen.de/en/news/documenta-fifteen-closes-with-very-good-attendance-figures/\" class=\"news-list__item\">\n",
        "                            <div class=\"news-list__date\">26.9.2022</div>\n",
        "                            <div class=\"news-list__textbox\">\n",
        "<h3 class=\"news-list__headline\">documenta fifteen closes with very good attendance figures </h3>\n",
        "<p class=\"news-list__informal\">General</p>\n",
        "                            </div>\n",
        "                        </a>\n",
        "                    </li>\n",
        "```"
      ],
      "metadata": {
        "id": "pPlnOwxkUST7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **second part** of the code will further analyse each news item to only extract the actual news content. In the source code, this content is hidden in the `<div class=\"news__content\">` HTML tag.\n",
        "\n",
        "The plain text extracted here will be written to one TXT file for item, and all TXT files will also be written to a separate folder on Google Drive."
      ],
      "metadata": {
        "id": "aubOzVBNUtm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYybHd19TKzS"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "\n",
        "import requests\n",
        "import urllib.request\n",
        "from urllib.parse import urljoin\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4.builder._lxml\n",
        "from xml.etree.ElementTree import XML, fromstring\n",
        "\n",
        "# URL to be called\n",
        "\n",
        "NEWS_url=\"https://documenta-fifteen.de/en/news/\"\n",
        "\n",
        "# function to extract html document from given url\n",
        "# as suggested on https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/\n",
        "\n",
        "def getHTMLdocument(url):\n",
        "      \n",
        "    # request for HTML document of given url\n",
        "    response1 = requests.get(url)\n",
        "      \n",
        "    # response will be provided in JSON format\n",
        "    return response1.text\n",
        "\n",
        "# Navigate to the application home page\n",
        "\n",
        "html_document = getHTMLdocument(NEWS_url)\n",
        "soup = BeautifulSoup(html_document, 'html')\n",
        "\n",
        "# find links for individual files\n",
        "\n",
        "links = soup.find_all(\"li\", {\"class\" : \"filter-list__item\"})\n",
        "\n",
        "# number of links found\n",
        "\n",
        "print(\"Number of links found: \", len(links))\n",
        "\n",
        "# create counter to number files\n",
        "\n",
        "counter=0\n",
        "no_links=len(links)\n",
        "\n",
        "# traverse list and get individual XML URLs\n",
        "\n",
        "try:\n",
        "    for lnk in links:\n",
        "        index=links.index(lnk)\n",
        "        counter=+index\n",
        "        print(counter)\n",
        "        l_tag = lnk.find(\"a\")\n",
        "        l_rel = l_tag.get(\"href\")\n",
        "        print(l_rel) \n",
        "\n",
        "    # use new URL to access individual HTML files\n",
        "    # get entire page content including metadata\n",
        "\n",
        "        response2 = requests.get(l_rel)\n",
        "        outfile=response2.text\n",
        "        \n",
        "    # generate new file names based on link names\n",
        "\n",
        "        new_l = l_rel.replace(\"https://documenta-fifteen.de/en/news/\", \"\")[:-1]\n",
        "        #print(new_l)\n",
        "\n",
        "    # get news content only\n",
        "\n",
        "        soup2 = BeautifulSoup(outfile, 'html')\n",
        "        news_content = soup2.find(\"div\", {\"class\" : \"news__content\"})\n",
        "        #print(news_content)\n",
        "        news_text = news_content.text\n",
        "        print(news_text) \n",
        "\n",
        "    # save each XML file to drive\n",
        "\n",
        "        with open((directory+\"Documenta15_XML/\" + new_l + '.html'), 'w') as f: # encoding=\"utf-8\"\n",
        "            print(f)\n",
        "            f.write(outfile)\n",
        "\n",
        "    # save news content to TXT files on drive\n",
        "\n",
        "        with open((directory+\"Documenta15_TXT/\" + new_l + '.txt'), 'w') as f: # encoding=\"utf-8\"\n",
        "            f.write(news_text)\n",
        "            print(\"File no.\", counter, \"downloaded!\")\n",
        "\n",
        "except AttributeError:\n",
        "    print(\"No more files on current page!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script provided by Monika Barget, Maastricht University, February 2022"
      ],
      "metadata": {
        "id": "KtixB_xzVYNE"
      }
    }
  ]
}